{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Colorization of Greyscale Images\n",
    "\n",
    "In this notebook, we will explore how to train a machine learning model with ``Keras`` and ``Tensorflow`` to colorize black and white images. Colorization has a wide variety of applications from restoring historical context and \"modernizing\" history to producing vibrant and colorful images which are served for recreational/personal purpose.\n",
    "\n",
    "\n",
    "![colorization](colorizingex.png)\n",
    "*This is an example from our pre-trained model integrated in Streamlit app (we got quite lucky with this image as you can see the result is almost perfect).*\n",
    "\n",
    "Okay! So that's cool and all. But what will we actually do to achieve such an impressive result? In short, for this project, we will:\n",
    "- Preprocess the image data\n",
    "- Build and train an convolutional neural network model combined with a classifier\n",
    "- Evaluate our model on the test set\n",
    "- Check out how our model performs\n",
    "\n",
    "**Note**: The algorithm from this notebook is based on and learned from Emil's blog post on colorization. For more detailed and thorough explanation of the process, please take a look at his amazing work on [medium](https://emilwallner.medium.com/colorize-b-w-photos-with-a-100-line-neural-network-53d9b4449f8d)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some Necessary Packages/Libraries for Data Processing and Machine Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-01T23:56:29.444947Z",
     "start_time": "2021-06-01T23:56:05.456132Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/blackbox/anaconda3/envs/colorizer/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:523: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/blackbox/anaconda3/envs/colorizer/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:524: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/blackbox/anaconda3/envs/colorizer/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/blackbox/anaconda3/envs/colorizer/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/blackbox/anaconda3/envs/colorizer/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/blackbox/anaconda3/envs/colorizer/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:532: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "import os\n",
    "import random\n",
    "import keras\n",
    "\n",
    "from keras.applications.inception_resnet_v2 import InceptionResNetV2\n",
    "from keras.applications.inception_resnet_v2 import preprocess_input\n",
    "\n",
    "from keras.preprocessing import image\n",
    "from keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img\n",
    "\n",
    "from keras.models import Sequential, Model\n",
    "from keras.callbacks import TensorBoard \n",
    "\n",
    "from keras.engine import Layer\n",
    "from keras.layers import Conv2D, UpSampling2D, InputLayer, Conv2DTranspose, Input, Reshape, merge, concatenate, Activation, Dense, Dropout, Flatten\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers.core import RepeatVector, Permute\n",
    "\n",
    "from skimage.color import rgb2lab, lab2rgb, rgb2gray, gray2rgb\n",
    "from skimage.transform import resize\n",
    "from skimage.io import imsave"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CPU v.s. GPU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The difference in computational power between a CPU and a GPU is extremely huge, and it's ideal to train this type of model on a GPU as it requires a very large computing power. From my rough calculation, it would take days to see some observable results if trained on a CPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "if tf.test.is_gpu_available():\n",
    "    # GPU -- this takes about 1-3 hours on Tesla K80 if the data set is about 20-50 images\n",
    "    # You will see some acceptable results for model trained with this paramters\n",
    "    BATCH_SIZE = 20\n",
    "    EPOCHS = 115\n",
    "else:\n",
    "    # CPU -- not recommended\n",
    "    BATCH_SIZE = 20\n",
    "    EPOCHS = 50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Sets\n",
    "For this project, I will be using a mix of images from sources mentioned in **Data/** folder and divide them into three sets: small (50 images), medium (200 images), big(10000 images).\n",
    "\n",
    "**Side Note**: When we first proposed the project, we were quite ambitious and adamant that we would train this model on roughly 500k-1000k images. Now looking back, given the available and limited resources we have (ram capacity and computational power <=> USD), such a task is impossible.\n",
    "\n",
    "\n",
    "After loading images, we basically first want to converting images into tensors and rescaling the pixel values from [0-255] to [0,1].\n",
    "\n",
    "Since there are lots images there, so I won't be uploading them to github, but instead you can find the sources to those in the **Data/** folder that's on my github."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add path to data sets\n",
    "DS_PATH = \"Data/TBA\"\n",
    "\n",
    "# Get images\n",
    "X = []\n",
    "for filename in os.listdir(DS_PATH):\n",
    "    if os.path.isfile(os.path.join(DS_PATH, filename)):\n",
    "        X.append(img_to_array(load_img(os.path.join(DS_PATH, filename))))\n",
    "                      \n",
    "# Normalization => Converting pixel value from [0-255] to [0,1]                      \n",
    "X = np.array(X, dtype=float)\n",
    "Xtrain = 1.0/255*X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's say we want to train our model on 20 images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20, 256, 256, 3)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check the shape\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can interpret the dimension of ``X`` as follows:\n",
    "- 20 : the number of images that would be used to trained our model\n",
    "- 256: width of the images\n",
    "- 256: height of the images\n",
    "- 3  : the 3 color channels in each of the image (RGB) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preprocessing\n",
    "\n",
    "We’ll use an algorithm to change the color channels, from RGB to Lab. L stands for lightness, and a and b for the color spectrums green–red and blue–yellow.\n",
    "\n",
    "\n",
    "We have a grayscale layer for input, and our ultimate goal is to predict two appropriate color layers, i.e., the ab in Lab.\n",
    "\n",
    "![Mapping from B&W to AB](https://miro.medium.com/max/700/1*W23SQ2oEdE_PsK-HmP4cow.png)\n",
    "*I found this picture from Emil's blog post sums up perfectly what we want to achieve*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from support import create_inception_embedding\n",
    "\n",
    "# Image transformer\n",
    "datagen = ImageDataGenerator(\n",
    "        shear_range=0.1,\n",
    "        zoom_range=0.1,\n",
    "        rotation_range=10,\n",
    "        horizontal_flip=True)\n",
    "\n",
    "def image_a_b_gen(batch_size):\n",
    "    \"\"\"\n",
    "    FUNCTION\n",
    "    ---------\n",
    "    Converts RGB images to B&W, extract the feature using Inception,\n",
    "    and get the LAB from the original image. \n",
    "    \n",
    "    INPUT\n",
    "    -----\n",
    "    batch_size (integer): the number of batch size\n",
    "    \n",
    "    OUTPUT\n",
    "    ------\n",
    "    Data for training the model later on\n",
    "    \"\"\"\n",
    "    for batch in datagen.flow(Xtrain, batch_size=batch_size):\n",
    "        # RGB to B&W\n",
    "        grayscaled_rgb = gray2rgb(rgb2gray(batch))\n",
    "        # Feature Extraction\n",
    "        embed = create_inception_embedding(inception, grayscaled_rgb)\n",
    "        # RGB to LAB\n",
    "        lab_batch = rgb2lab(batch)\n",
    "        X_batch = lab_batch[:,:,:,0]\n",
    "        X_batch = X_batch.reshape(X_batch.shape+(1,))\n",
    "        # Convert LAB value from [-128, 128] to [-1, 1]\n",
    "        Y_batch = lab_batch[:,:,:,1:] / 128\n",
    "        # The new Batch (B&W, Embedding, LAB)\n",
    "        yield ([X_batch, create_inception_embedding(inception, grayscaled_rgb)], Y_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model\n",
    "\n",
    "In his blog post, Emil used a model from Federico Baldassarre’s [Deep Koalarization: Image Colorization using CNNs and Inception-ResNet-v2](https://arxiv.org/abs/1712.03400) which, in principle, works as follows:\n",
    "\n",
    "![colornet](https://raw.githubusercontent.com/baldassarreFe/deep-koalarization/master/assets/our_net.png)\n",
    "*Deep Koalarization: Image Colorization using CNN and Inception-ResNet-v2 -- Image from [the paper](https://arxiv.org/abs/1712.03400)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load weights of InceptionResNet model for embedding extraction \n",
    "inception = InceptionResNetV2(weights=None, include_top=True)\n",
    "inception.load_weights('../Models/inception_resnet_v2_weights_tf_dim_ordering_tf_kernels.h5')\n",
    "inception.graph = tf.get_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_3 (InputLayer)            (None, 256, 256, 1)  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_204 (Conv2D)             (None, 128, 128, 64) 640         input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_205 (Conv2D)             (None, 128, 128, 128 73856       conv2d_204[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_206 (Conv2D)             (None, 64, 64, 128)  147584      conv2d_205[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_207 (Conv2D)             (None, 64, 64, 256)  295168      conv2d_206[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_208 (Conv2D)             (None, 32, 32, 256)  590080      conv2d_207[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_209 (Conv2D)             (None, 32, 32, 512)  1180160     conv2d_208[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            (None, 1000)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_210 (Conv2D)             (None, 32, 32, 512)  2359808     conv2d_209[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "repeat_vector_1 (RepeatVector)  (None, 1024, 1000)   0           input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_211 (Conv2D)             (None, 32, 32, 256)  1179904     conv2d_210[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "reshape_1 (Reshape)             (None, 32, 32, 1000) 0           repeat_vector_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 32, 32, 1256) 0           conv2d_211[0][0]                 \n",
      "                                                                 reshape_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_212 (Conv2D)             (None, 32, 32, 256)  321792      concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_213 (Conv2D)             (None, 32, 32, 128)  295040      conv2d_212[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling2d_1 (UpSampling2D)  (None, 64, 64, 128)  0           conv2d_213[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_214 (Conv2D)             (None, 64, 64, 64)   73792       up_sampling2d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling2d_2 (UpSampling2D)  (None, 128, 128, 64) 0           conv2d_214[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_215 (Conv2D)             (None, 128, 128, 32) 18464       up_sampling2d_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_216 (Conv2D)             (None, 128, 128, 16) 4624        conv2d_215[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_217 (Conv2D)             (None, 128, 128, 2)  130         conv2d_216[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling2d_3 (UpSampling2D)  (None, 256, 256, 2)  0           conv2d_217[0][0]                 \n",
      "==================================================================================================\n",
      "Total params: 6,541,042\n",
      "Trainable params: 6,541,042\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# The Model\n",
    "def conv_stack(data, filters, s):\n",
    "    # utility to build convolutional layers\n",
    "    output = Conv2D(filters, (3, 3), strides=s, activation='relu', padding='same')(data)\n",
    "    return output\n",
    "\n",
    "embed_input = Input(shape=(1000,))\n",
    "\n",
    "#Encoder\n",
    "encoder_input = Input(shape=(256, 256, 1,))\n",
    "encoder_output = conv_stack(encoder_input, 64, 2)\n",
    "encoder_output = conv_stack(encoder_output, 128, 1)\n",
    "encoder_output = conv_stack(encoder_output, 128, 2)\n",
    "encoder_output = conv_stack(encoder_output, 256, 1)\n",
    "encoder_output = conv_stack(encoder_output, 256, 2)\n",
    "encoder_output = conv_stack(encoder_output, 512, 1)\n",
    "encoder_output = conv_stack(encoder_output, 512, 1)\n",
    "encoder_output = conv_stack(encoder_output, 256, 1)\n",
    "\n",
    "#Fusion\n",
    "fusion_output = RepeatVector(32 * 32)(embed_input) \n",
    "fusion_output = Reshape(([32, 32, 1000]))(fusion_output)\n",
    "fusion_output = concatenate([encoder_output, fusion_output], axis=3) \n",
    "fusion_output = Conv2D(256, (1, 1), activation='relu')(fusion_output) \n",
    "\n",
    "\n",
    "\n",
    "#Decoder\n",
    "decoder_output = conv_stack(fusion_output, 128, 1)\n",
    "decoder_output = UpSampling2D((2, 2))(decoder_output)\n",
    "decoder_output = conv_stack(decoder_output, 64, 1)\n",
    "decoder_output = UpSampling2D((2, 2))(decoder_output)\n",
    "decoder_output = conv_stack(decoder_output, 32, 1)\n",
    "decoder_output = conv_stack(decoder_output, 16, 1)\n",
    "decoder_output = Conv2D(2, (2, 2), activation='tanh', padding='same')(decoder_output)\n",
    "decoder_output = UpSampling2D((2, 2))(decoder_output)\n",
    "\n",
    "model = Model(inputs=[encoder_input, embed_input], outputs=decoder_output)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wow! What a massive number of parameters! Again, this solidifies what we recommend the readers earlier when training with this model. **Always prioritize GPU!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Time\n",
    "Before proceeding, it's a good thing to be aware that this step would require a lot of dedication, patient and especially time. So what does I mean by making such a statement? Well, it turns out that you need to fine-tune different parameters here such as the number of epochs, batch size, or how many images to train with, etc, to get the desired result, and this process takes quite a lot of time and perseverance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/115\n",
      "10/10 [==============================] - 41s 4s/step - loss: 0.0339 - acc: 0.4748\n",
      "Epoch 2/115\n",
      "10/10 [==============================] - 32s 3s/step - loss: 0.0115 - acc: 0.5206\n",
      "Epoch 3/115\n",
      "10/10 [==============================] - 30s 3s/step - loss: 0.0115 - acc: 0.5765\n",
      "Epoch 4/115\n",
      "10/10 [==============================] - 30s 3s/step - loss: 0.0116 - acc: 0.5717\n",
      "Epoch 5/115\n",
      "10/10 [==============================] - 30s 3s/step - loss: 0.0115 - acc: 0.5760\n",
      "Epoch 6/115\n",
      "10/10 [==============================] - 30s 3s/step - loss: 0.0114 - acc: 0.5749\n",
      "Epoch 7/115\n",
      "10/10 [==============================] - 30s 3s/step - loss: 0.0113 - acc: 0.5830\n",
      "Epoch 8/115\n",
      "10/10 [==============================] - 30s 3s/step - loss: 0.0112 - acc: 0.5818\n",
      "Epoch 9/115\n",
      "10/10 [==============================] - 30s 3s/step - loss: 0.0113 - acc: 0.5798\n",
      "Epoch 10/115\n",
      "10/10 [==============================] - 30s 3s/step - loss: 0.0114 - acc: 0.5831\n",
      "Epoch 11/115\n",
      "10/10 [==============================] - 30s 3s/step - loss: 0.0115 - acc: 0.5825\n",
      "Epoch 12/115\n",
      "10/10 [==============================] - 32s 3s/step - loss: 0.0119 - acc: 0.5691\n",
      "Epoch 13/115\n",
      "10/10 [==============================] - 32s 3s/step - loss: 0.0113 - acc: 0.5892\n",
      "Epoch 14/115\n",
      "10/10 [==============================] - 31s 3s/step - loss: 0.0112 - acc: 0.5781\n",
      "Epoch 15/115\n",
      "10/10 [==============================] - 31s 3s/step - loss: 0.0110 - acc: 0.5880\n",
      "Epoch 16/115\n",
      "10/10 [==============================] - 31s 3s/step - loss: 0.0112 - acc: 0.5799\n",
      "Epoch 17/115\n",
      "10/10 [==============================] - 30s 3s/step - loss: 0.0108 - acc: 0.5876\n",
      "Epoch 18/115\n",
      "10/10 [==============================] - 30s 3s/step - loss: 0.0105 - acc: 0.6087\n",
      "Epoch 19/115\n",
      "10/10 [==============================] - 30s 3s/step - loss: 0.0101 - acc: 0.6109\n",
      "Epoch 20/115\n",
      "10/10 [==============================] - 30s 3s/step - loss: 0.0097 - acc: 0.6090\n",
      "Epoch 21/115\n",
      "10/10 [==============================] - 30s 3s/step - loss: 0.0096 - acc: 0.6033\n",
      "Epoch 22/115\n",
      "10/10 [==============================] - 31s 3s/step - loss: 0.0091 - acc: 0.6145\n",
      "Epoch 23/115\n",
      "10/10 [==============================] - 31s 3s/step - loss: 0.0086 - acc: 0.6168\n",
      "Epoch 24/115\n",
      "10/10 [==============================] - 31s 3s/step - loss: 0.0081 - acc: 0.6272\n",
      "Epoch 25/115\n",
      "10/10 [==============================] - 31s 3s/step - loss: 0.0077 - acc: 0.6352\n",
      "Epoch 26/115\n",
      "10/10 [==============================] - 31s 3s/step - loss: 0.0073 - acc: 0.6427\n",
      "Epoch 27/115\n",
      "10/10 [==============================] - 31s 3s/step - loss: 0.0078 - acc: 0.6302\n",
      "Epoch 28/115\n",
      "10/10 [==============================] - 31s 3s/step - loss: 0.0073 - acc: 0.6475\n",
      "Epoch 29/115\n",
      "10/10 [==============================] - 31s 3s/step - loss: 0.0064 - acc: 0.6518\n",
      "Epoch 30/115\n",
      "10/10 [==============================] - 31s 3s/step - loss: 0.0064 - acc: 0.6608\n",
      "Epoch 31/115\n",
      "10/10 [==============================] - 31s 3s/step - loss: 0.0057 - acc: 0.6693\n",
      "Epoch 32/115\n",
      "10/10 [==============================] - 32s 3s/step - loss: 0.0055 - acc: 0.6821\n",
      "Epoch 33/115\n",
      "10/10 [==============================] - 32s 3s/step - loss: 0.0049 - acc: 0.6942\n",
      "Epoch 34/115\n",
      "10/10 [==============================] - 31s 3s/step - loss: 0.0045 - acc: 0.6977\n",
      "Epoch 35/115\n",
      "10/10 [==============================] - 31s 3s/step - loss: 0.0042 - acc: 0.7086\n",
      "Epoch 36/115\n",
      "10/10 [==============================] - 31s 3s/step - loss: 0.0041 - acc: 0.7115\n",
      "Epoch 37/115\n",
      "10/10 [==============================] - 31s 3s/step - loss: 0.0048 - acc: 0.7039\n",
      "Epoch 38/115\n",
      "10/10 [==============================] - 31s 3s/step - loss: 0.0039 - acc: 0.7161\n",
      "Epoch 39/115\n",
      "10/10 [==============================] - 31s 3s/step - loss: 0.0038 - acc: 0.7222\n",
      "Epoch 40/115\n",
      "10/10 [==============================] - 31s 3s/step - loss: 0.0036 - acc: 0.7294\n",
      "Epoch 41/115\n",
      "10/10 [==============================] - 31s 3s/step - loss: 0.0034 - acc: 0.7337\n",
      "Epoch 42/115\n",
      "10/10 [==============================] - 31s 3s/step - loss: 0.0031 - acc: 0.7450\n",
      "Epoch 43/115\n",
      "10/10 [==============================] - 32s 3s/step - loss: 0.0029 - acc: 0.7525\n",
      "Epoch 44/115\n",
      "10/10 [==============================] - 32s 3s/step - loss: 0.0031 - acc: 0.7491\n",
      "Epoch 45/115\n",
      "10/10 [==============================] - 32s 3s/step - loss: 0.0029 - acc: 0.7469\n",
      "Epoch 46/115\n",
      "10/10 [==============================] - 32s 3s/step - loss: 0.0026 - acc: 0.7614\n",
      "Epoch 47/115\n",
      "10/10 [==============================] - 31s 3s/step - loss: 0.0038 - acc: 0.7405\n",
      "Epoch 48/115\n",
      "10/10 [==============================] - 32s 3s/step - loss: 0.0033 - acc: 0.7435\n",
      "Epoch 49/115\n",
      "10/10 [==============================] - 31s 3s/step - loss: 0.0026 - acc: 0.7649\n",
      "Epoch 50/115\n",
      "10/10 [==============================] - 31s 3s/step - loss: 0.0024 - acc: 0.7711\n",
      "Epoch 51/115\n",
      "10/10 [==============================] - 31s 3s/step - loss: 0.0023 - acc: 0.7745\n",
      "Epoch 52/115\n",
      "10/10 [==============================] - 31s 3s/step - loss: 0.0022 - acc: 0.7826\n",
      "Epoch 53/115\n",
      "10/10 [==============================] - 31s 3s/step - loss: 0.0022 - acc: 0.7849\n",
      "Epoch 54/115\n",
      "10/10 [==============================] - 31s 3s/step - loss: 0.0020 - acc: 0.7892\n",
      "Epoch 55/115\n",
      "10/10 [==============================] - 31s 3s/step - loss: 0.0022 - acc: 0.7860\n",
      "Epoch 56/115\n",
      "10/10 [==============================] - 33s 3s/step - loss: 0.0020 - acc: 0.7959\n",
      "Epoch 57/115\n",
      "10/10 [==============================] - 31s 3s/step - loss: 0.0020 - acc: 0.7977\n",
      "Epoch 58/115\n",
      "10/10 [==============================] - 31s 3s/step - loss: 0.0021 - acc: 0.7940\n",
      "Epoch 59/115\n",
      "10/10 [==============================] - 31s 3s/step - loss: 0.0019 - acc: 0.7968\n",
      "Epoch 60/115\n",
      "10/10 [==============================] - 32s 3s/step - loss: 0.0018 - acc: 0.8088\n",
      "Epoch 61/115\n",
      "10/10 [==============================] - 31s 3s/step - loss: 0.0017 - acc: 0.8148\n",
      "Epoch 62/115\n",
      "10/10 [==============================] - 31s 3s/step - loss: 0.0017 - acc: 0.8154\n",
      "Epoch 63/115\n",
      "10/10 [==============================] - 33s 3s/step - loss: 0.0018 - acc: 0.8144\n",
      "Epoch 64/115\n",
      "10/10 [==============================] - 31s 3s/step - loss: 0.0021 - acc: 0.7933\n",
      "Epoch 65/115\n",
      "10/10 [==============================] - 31s 3s/step - loss: 0.0018 - acc: 0.8071\n",
      "Epoch 66/115\n",
      "10/10 [==============================] - 37s 4s/step - loss: 0.0016 - acc: 0.8206\n",
      "Epoch 67/115\n",
      "10/10 [==============================] - 42s 4s/step - loss: 0.0015 - acc: 0.8281\n",
      "Epoch 68/115\n",
      "10/10 [==============================] - 42s 4s/step - loss: 0.0015 - acc: 0.8301\n",
      "Epoch 69/115\n",
      "10/10 [==============================] - 35s 4s/step - loss: 0.0015 - acc: 0.8342\n",
      "Epoch 70/115\n",
      "10/10 [==============================] - 31s 3s/step - loss: 0.0014 - acc: 0.8374\n",
      "Epoch 71/115\n",
      "10/10 [==============================] - 31s 3s/step - loss: 0.0014 - acc: 0.8405\n",
      "Epoch 72/115\n",
      "10/10 [==============================] - 31s 3s/step - loss: 0.0014 - acc: 0.8407\n",
      "Epoch 73/115\n",
      "10/10 [==============================] - 31s 3s/step - loss: 0.0013 - acc: 0.8448\n",
      "Epoch 74/115\n",
      "10/10 [==============================] - 31s 3s/step - loss: 0.0013 - acc: 0.8466\n",
      "Epoch 75/115\n",
      "10/10 [==============================] - 31s 3s/step - loss: 0.0014 - acc: 0.8469\n",
      "Epoch 76/115\n",
      "10/10 [==============================] - 31s 3s/step - loss: 0.0014 - acc: 0.8468\n",
      "Epoch 77/115\n",
      "10/10 [==============================] - 31s 3s/step - loss: 0.0013 - acc: 0.8506\n",
      "Epoch 78/115\n",
      "10/10 [==============================] - 30s 3s/step - loss: 0.0013 - acc: 0.8502\n",
      "Epoch 79/115\n",
      "10/10 [==============================] - 30s 3s/step - loss: 0.0013 - acc: 0.8504\n",
      "Epoch 80/115\n",
      "10/10 [==============================] - 30s 3s/step - loss: 0.0013 - acc: 0.8529\n",
      "Epoch 81/115\n",
      "10/10 [==============================] - 30s 3s/step - loss: 0.0012 - acc: 0.8513\n",
      "Epoch 82/115\n",
      "10/10 [==============================] - 31s 3s/step - loss: 0.0012 - acc: 0.8561\n",
      "Epoch 83/115\n",
      "10/10 [==============================] - 30s 3s/step - loss: 0.0012 - acc: 0.8580\n",
      "Epoch 84/115\n",
      "10/10 [==============================] - 31s 3s/step - loss: 0.0012 - acc: 0.8576\n",
      "Epoch 85/115\n",
      "10/10 [==============================] - 30s 3s/step - loss: 0.0012 - acc: 0.8574\n",
      "Epoch 86/115\n",
      "10/10 [==============================] - 31s 3s/step - loss: 0.0012 - acc: 0.8616\n",
      "Epoch 87/115\n",
      "10/10 [==============================] - 31s 3s/step - loss: 0.0011 - acc: 0.8625\n",
      "Epoch 88/115\n",
      "10/10 [==============================] - 31s 3s/step - loss: 0.0011 - acc: 0.8652\n",
      "Epoch 89/115\n",
      "10/10 [==============================] - 30s 3s/step - loss: 0.0011 - acc: 0.8663\n",
      "Epoch 90/115\n",
      "10/10 [==============================] - 30s 3s/step - loss: 0.0011 - acc: 0.8652\n",
      "Epoch 91/115\n",
      "10/10 [==============================] - 31s 3s/step - loss: 0.0011 - acc: 0.8699\n",
      "Epoch 92/115\n",
      "10/10 [==============================] - 31s 3s/step - loss: 0.0011 - acc: 0.8664\n",
      "Epoch 93/115\n",
      "10/10 [==============================] - 30s 3s/step - loss: 0.0011 - acc: 0.8694\n",
      "Epoch 94/115\n",
      "10/10 [==============================] - 30s 3s/step - loss: 0.0011 - acc: 0.8693\n",
      "Epoch 95/115\n",
      "10/10 [==============================] - 30s 3s/step - loss: 0.0011 - acc: 0.8657\n",
      "Epoch 96/115\n",
      "10/10 [==============================] - 30s 3s/step - loss: 0.0011 - acc: 0.8676\n",
      "Epoch 97/115\n",
      "10/10 [==============================] - 31s 3s/step - loss: 0.0011 - acc: 0.8679\n",
      "Epoch 98/115\n",
      "10/10 [==============================] - 31s 3s/step - loss: 0.0011 - acc: 0.8693\n",
      "Epoch 99/115\n",
      "10/10 [==============================] - 30s 3s/step - loss: 0.0010 - acc: 0.8720\n",
      "Epoch 100/115\n",
      "10/10 [==============================] - 31s 3s/step - loss: 0.0010 - acc: 0.8725\n",
      "Epoch 101/115\n",
      "10/10 [==============================] - 30s 3s/step - loss: 0.0010 - acc: 0.8727\n",
      "Epoch 102/115\n",
      "10/10 [==============================] - 31s 3s/step - loss: 0.0010 - acc: 0.8730\n",
      "Epoch 103/115\n",
      "10/10 [==============================] - 31s 3s/step - loss: 0.0010 - acc: 0.8738\n",
      "Epoch 104/115\n",
      "10/10 [==============================] - 31s 3s/step - loss: 9.8816e-04 - acc: 0.8750\n",
      "Epoch 105/115\n",
      "10/10 [==============================] - 30s 3s/step - loss: 9.9116e-04 - acc: 0.8749\n",
      "Epoch 106/115\n",
      "10/10 [==============================] - 30s 3s/step - loss: 9.8176e-04 - acc: 0.8737\n",
      "Epoch 107/115\n",
      "10/10 [==============================] - 30s 3s/step - loss: 9.8142e-04 - acc: 0.8729\n",
      "Epoch 108/115\n",
      "10/10 [==============================] - 31s 3s/step - loss: 9.8116e-04 - acc: 0.8723\n",
      "Epoch 109/115\n",
      "10/10 [==============================] - 31s 3s/step - loss: 9.7492e-04 - acc: 0.8774\n",
      "Epoch 110/115\n",
      "10/10 [==============================] - 30s 3s/step - loss: 0.0010 - acc: 0.8733\n",
      "Epoch 111/115\n",
      "10/10 [==============================] - 30s 3s/step - loss: 0.0010 - acc: 0.8701\n",
      "Epoch 112/115\n",
      "10/10 [==============================] - 31s 3s/step - loss: 9.9864e-04 - acc: 0.8709\n",
      "Epoch 113/115\n",
      "10/10 [==============================] - 30s 3s/step - loss: 9.6097e-04 - acc: 0.8787\n",
      "Epoch 114/115\n",
      "10/10 [==============================] - 30s 3s/step - loss: 9.4463e-04 - acc: 0.8789\n",
      "Epoch 115/115\n",
      "10/10 [==============================] - 30s 3s/step - loss: 9.5235e-04 - acc: 0.8765\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f40e03bf908>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Train model \n",
    "tensorboard = TensorBoard(log_dir=\"/\")\n",
    "model.compile(optimizer='adam', loss='mse', metrics=['accuracy'])\n",
    "model.fit_generator(image_a_b_gen(BATCH_SIZE), \n",
    "                    callbacks=[tensorboard],\n",
    "                    epochs=EPOCHS, steps_per_epoch=10, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-02T00:54:38.032388Z",
     "start_time": "2021-06-02T00:48:50.465586Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pre-trained model...\n",
      "Model loaded!\n"
     ]
    }
   ],
   "source": [
    "# Evaluate Colorization\n",
    "from support import load_pretrained_model\n",
    "from outputImg import color_result\n",
    "\n",
    "(model, inception) = load_pretrained_model('../Models/inception_resnet_v2_weights_tf_dim_ordering_tf_kernels.h5',\n",
    "                                           '../Models/color_tensorflow_ds_small_115.h5')\n",
    "\n",
    "START = 0\n",
    "END = 7\n",
    "PATH = 'Test/'\n",
    "RESULT = 'Result'\n",
    "\n",
    "color_result(PATH, START, END, RESULT, model, inception)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-02T05:26:35.081209Z",
     "start_time": "2021-06-02T05:26:30.725877Z"
    },
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1b36e1b9936747a98c2e205375a6e84d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=1, description='Results', max=6, min=1), Output()), _dom_classes=('widge…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Show results\n",
    "from ipywidgets import interact\n",
    "from ipywidgets import widgets\n",
    "from outputImg import show_img \n",
    "\n",
    "def show_sample(sample_n):\n",
    "    \"\"\"\n",
    "    showcase black and white images which got colorized by our model\n",
    "    \"\"\"\n",
    "    image_path = os.path.join(RESULT, \"img_\"+str(sample_n-1)+\".png\")\n",
    "    img = image.load_img(image_path)\n",
    "    img = image.img_to_array(img)/255\n",
    "    ax = show_img(img, figsize=(9,9))\n",
    "    ax.set_title(image_path)\n",
    "    \n",
    "interact(show_sample, sample_n=widgets.IntSlider(value=1, min=1, max=END-START-1, description='Results'));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also try the model on images that we get from the web. In addition, we will showcase here how well our model performs compared to the original colored version of an image (if applicable).\n",
    "\n",
    "Some colored photos for demo:\n",
    "- https://cdn.pixabay.com/photo/2017/04/07/18/23/landscape-2211587_960_720.jpg\n",
    "- https://photographycourse.net/wp-content/uploads/2014/11/Landscape-Photography-steps.jpg\n",
    "- https://briansmith.com/wp-content/uploads/2012/02/Brian-Smith-Samuel-L-Jackson.jpg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-02T05:27:08.212822Z",
     "start_time": "2021-06-02T05:27:08.005483Z"
    },
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f6226e5ccaf43dba9da58b8de32edae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(Text(value='', description='URL', placeholder='Insert an image URL'), Button(description…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Testing on url images\n",
    "from ipywidgets import interact_manual\n",
    "from ipywidgets import widgets\n",
    "from support import prediction_from_url\n",
    "\n",
    "def get_prediction(URL):\n",
    "    prediction_from_url(URL, model, inception)\n",
    "\n",
    "interact_manual(get_prediction, URL=widgets.Text(placeholder='Insert an image URL'));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save the Trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_weights(\"../Models/model_{}_epochs.h5\".format(EPOCHS))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
